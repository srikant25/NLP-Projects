{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.78\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      Analyze       0.87      0.87      0.87        23\n",
      "  Application       0.89      0.77      0.83        31\n",
      "Comprehension       0.86      0.78      0.82        32\n",
      "   evaluation       0.59      0.64      0.62        25\n",
      "    knowledge       0.74      0.88      0.80        16\n",
      "     synthesi       0.72      0.78      0.75        23\n",
      "\n",
      "     accuracy                           0.78       150\n",
      "    macro avg       0.78      0.79      0.78       150\n",
      " weighted avg       0.79      0.78      0.78       150\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_data=pd.read_csv('C:/Users/SRIKANT NAYAK/Documents/code/bloom_texonomy/data2.csv',encoding= 'unicode_escape')\n",
    "\n",
    "new_data=pd.read_csv('C:/Users/SRIKANT NAYAK/Documents/code/bloom_texonomy/upsc.csv',encoding= 'unicode_escape')\n",
    "\n",
    "\n",
    "\n",
    "def Question_classification(training_data,new_data):\n",
    " \n",
    "   #...........................................import the libreries....................................................\n",
    "    \n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import re\n",
    "    import nltk\n",
    "    import matplotlib.pyplot as plt\n",
    "    import re\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.stem import PorterStemmer\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.metrics import classification_report, confusion_matrix,accuracy_score\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.feature_selection import SelectKBest, chi2\n",
    "    from sqlite3 import Error\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    import sqlite3\n",
    "    import pickle\n",
    "    \n",
    "    #........................................concartinate the training data and the new data.................................\n",
    "    \n",
    "    wpt = nltk.WordPunctTokenizer()\n",
    "    stop_words = nltk.corpus.stopwords.words('english')\n",
    "    \n",
    "    df=pd.DataFrame(training_data)\n",
    "    inputt=df.iloc[0:,0:-1]\n",
    "    labels=df.Label\n",
    "    \n",
    "    new_data=(pd.DataFrame(new_data))\n",
    "\n",
    "    frame = [inputt,new_data]\n",
    "    \n",
    "    total_training_data=pd.concat(frame, axis=0, join='outer', ignore_index=False, keys=None,levels=None, names=None, verify_integrity=False, copy=True)\n",
    "\n",
    "    \n",
    "    #...........................................preprocessing the data.........................................................\n",
    "    \n",
    "    \n",
    "\n",
    "    def normalize_document(doc):\n",
    "        # lower case and remove special characters\\whitespaces\n",
    "        doc = re.sub(r'[^a-zA-Z\\s]', '', doc, re.I|re.A)\n",
    "        doc = doc.lower()\n",
    "        doc = doc.strip()\n",
    "        # tokenize document\n",
    "        tokens = wpt.tokenize(doc)\n",
    "        # filter stopwords out of document\n",
    "        filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "        # re-create document from filtered tokens\n",
    "        doc = ' '.join(filtered_tokens)\n",
    "        return doc\n",
    "\n",
    "    normalize_corpus = np.vectorize(normalize_document)\n",
    "    norm_total_training_data = normalize_corpus(total_training_data)\n",
    "    \n",
    "   \n",
    "   #......................................convert into vector form................................................... \n",
    "    \n",
    "    def vectors(norm_corpus):\n",
    "        from sklearn.feature_extraction.text import CountVectorizer\n",
    "        cv = CountVectorizer(min_df=0., max_df=1.)\n",
    "        cv_matrix = cv.fit_transform(norm_corpus.ravel())\n",
    "        cv_matrix = cv_matrix.toarray()\n",
    "        # get all unique words in the corpus\n",
    "        vocab = cv.get_feature_names()\n",
    "        # show document feature vectors\n",
    "        pd.DataFrame(cv_matrix, columns=vocab)\n",
    "        # you can set the n-gram range to 1,2 to get unigrams as well as bigrams\n",
    "        bv = CountVectorizer(ngram_range=(2,2))\n",
    "        bv_matrix = bv.fit_transform(norm_corpus.ravel())\n",
    "\n",
    "        bv_matrix = bv_matrix.toarray()\n",
    "        vocab = bv.get_feature_names()\n",
    "        pd.DataFrame(bv_matrix, columns=vocab)\n",
    "        from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "        tv = TfidfVectorizer(min_df=0., max_df=1., use_idf=True)\n",
    "        tv_matrix = tv.fit_transform(norm_corpus.ravel())\n",
    "        tv_matrix = tv_matrix.toarray()\n",
    "\n",
    "        vocab = tv.get_feature_names()\n",
    "        pd.DataFrame(np.round(tv_matrix, 2), columns=vocab)\n",
    "        from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "        similarity_matrix = cosine_similarity(tv_matrix)\n",
    "        similarity_df = pd.DataFrame(similarity_matrix)\n",
    "        return similarity_df\n",
    "    \n",
    "    vector_norm_total_training_data = vectors(norm_total_training_data)\n",
    "    \n",
    "    X=vector_norm_total_training_data[0:len(data)]\n",
    "    \n",
    "    new_question=vector_norm_total_training_data[len(data):]\n",
    "    \n",
    "\n",
    "   # ................................................training the model.....................................................\n",
    "\n",
    "    #X = df['cleaned']\n",
    "    Y = df['Label']\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.25)\n",
    "    # instead of doing these steps one at a time, we can use a pipeline to complete them all at once\n",
    "    from sklearn.svm import LinearSVC\n",
    "    def support_vector_machine(X_train, y, X_predict):\n",
    "        lin_clf = LinearSVC()\n",
    "        lin_clf.fit(X_train, y)\n",
    "        prediction = lin_clf.predict(X_predict)\n",
    "        return prediction\n",
    "\n",
    "    y_pred=support_vector_machine(X_train,y_train,X_test)\n",
    "    #xtest=pca.transform(x_test)\n",
    "    from sklearn.metrics import classification_report,accuracy_score\n",
    "    #y_pred=cls.predict(vectorization.transform(xtest))\n",
    "    print(accuracy_score(y_test,y_pred))\n",
    "    print(classification_report(y_test,y_pred))\n",
    "    class_pred=support_vector_machine(X_train,y_train,new_question)\n",
    "    \n",
    "    return class_pred\n",
    "\n",
    "new_question_class_pred=Question_classification(training_data,new_data)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Application', 'Analyze', 'knowledge', 'evaluation', 'synthesi',\n",
       "       'knowledge', 'evaluation', 'knowledge', 'knowledge', 'knowledge',\n",
       "       'evaluation', 'knowledge', 'Analyze', 'Application', 'evaluation',\n",
       "       'Analyze', 'Analyze', 'knowledge', 'knowledge', 'Analyze'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(class_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "label=pd.DataFrame(class_pred,columns=['Label'])\n",
    "frame = [upsc,label]\n",
    "final_test_data=pd.concat(frame, axis=1, join='outer', ignore_index=False, keys=None,levels=None, names=None, verify_integrity=False, copy=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The 1857 Uprising was the culmination of the r...</td>\n",
       "      <td>Application</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Examine the linkages between the ninteenth cen...</td>\n",
       "      <td>Analyze</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Assess the impact of global warming on the cor...</td>\n",
       "      <td>knowledge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Discuss the causes of depletion of mangroves a...</td>\n",
       "      <td>evaluation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Can the strategy of regional resource-based ma...</td>\n",
       "      <td>synthesi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Discuss the factors for localisation of agro-b...</td>\n",
       "      <td>knowledge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>What makes the Indian society unique in sustai...</td>\n",
       "      <td>evaluation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>\"Empowering women is the key to control popula...</td>\n",
       "      <td>knowledge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>What are the challenges to our cultural practi...</td>\n",
       "      <td>knowledge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Many voices had strengthened and enriched the ...</td>\n",
       "      <td>knowledge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Assess the role of British imperial power in c...</td>\n",
       "      <td>evaluation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Explain how the foundations of the modern worl...</td>\n",
       "      <td>knowledge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>What is water stress? How and why does it diff...</td>\n",
       "      <td>Analyze</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>How can the mountain ecosystem be restored fro...</td>\n",
       "      <td>Application</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>How is efficient and affordable urban mass tra...</td>\n",
       "      <td>evaluation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>How do ocean currents and water masses differ ...</td>\n",
       "      <td>Analyze</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Do we have cultural pockets of small India all...</td>\n",
       "      <td>Analyze</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>What are the continued challenges for women in...</td>\n",
       "      <td>knowledge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Are we losing your local identity for the glob...</td>\n",
       "      <td>knowledge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Highlight the Central Asian and Greco-Bactrian...</td>\n",
       "      <td>Analyze</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Question        Label\n",
       "0   The 1857 Uprising was the culmination of the r...  Application\n",
       "1   Examine the linkages between the ninteenth cen...      Analyze\n",
       "2   Assess the impact of global warming on the cor...    knowledge\n",
       "3   Discuss the causes of depletion of mangroves a...   evaluation\n",
       "4   Can the strategy of regional resource-based ma...     synthesi\n",
       "5   Discuss the factors for localisation of agro-b...    knowledge\n",
       "6   What makes the Indian society unique in sustai...   evaluation\n",
       "7   \"Empowering women is the key to control popula...    knowledge\n",
       "8   What are the challenges to our cultural practi...    knowledge\n",
       "9   Many voices had strengthened and enriched the ...    knowledge\n",
       "10  Assess the role of British imperial power in c...   evaluation\n",
       "11  Explain how the foundations of the modern worl...    knowledge\n",
       "12  What is water stress? How and why does it diff...      Analyze\n",
       "13  How can the mountain ecosystem be restored fro...  Application\n",
       "14  How is efficient and affordable urban mass tra...   evaluation\n",
       "15  How do ocean currents and water masses differ ...      Analyze\n",
       "16  Do we have cultural pockets of small India all...      Analyze\n",
       "17  What are the continued challenges for women in...    knowledge\n",
       "18  Are we losing your local identity for the glob...    knowledge\n",
       "19  Highlight the Central Asian and Greco-Bactrian...      Analyze"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the output in csv file\n",
    "\n",
    "final_test_data.to_csv(\"upsc_classification.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
